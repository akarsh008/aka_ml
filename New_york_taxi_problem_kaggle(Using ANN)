import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
from keras import optimizers
from keras import regularizers

def clean(df):
    # Delimiter lats and lons to NY only
    df=df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]
    df=df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]
    df=df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]
    df=df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]
    # Remove possible outliers
    df=df[(1 <= df['passenger_count']) & (df['passenger_count'] <= 6)]
    df=df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]    
    df=df[(df['dropoff_longitude'] != df['pickup_longitude'])]
    df=df[(df['dropoff_latitude'] != df['pickup_latitude'])]
    
    return df

def late_night (row):
    if (row['hour'] <= 6) or (row['hour'] >= 20):
        return 1
    else:
        return 0


def night (row):
    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):
        return 1
    else:
        return 0
    
def process(df):
    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')
    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)
    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)
    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)
    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)
    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())
    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))
    df['night'] = df.apply (lambda x: night(x), axis=1)
    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)
    
    return df

def add_engineered(df):
    lat1 = df['pickup_latitude']
    lat2 = df['dropoff_latitude']
    lon1 = df['pickup_longitude']
    lon2 = df['dropoff_longitude']
    weekday = df['weekday']
    hour = df['hour']
    
    latdiff = (lat1 - lat2)
    londiff = (lon1 - lon2)
    euclidean = (latdiff ** 2 + londiff ** 2) ** 0.5
    ploc = lat1 * lon1
    dloc = lat2 * lon2
    pd_pair = ploc * dloc
    day_hr = weekday * hour

    # Add new features
    df['latdiff'] = latdiff
    df['londiff'] = londiff
    df['euclidean'] = euclidean
    df['ploc'] = ploc
    df['dloc'] = dloc
    df['pd_pair'] = pd_pair
    df['day_hr'] = day_hr        

# One-hot encoding columns
    # Note, this is note the best way to one-hot encode features, but probably the simplest and will work here
    df = pd.get_dummies(df, columns=['weekday'])
    df = pd.get_dummies(df, columns=['month'])
    
    return df

def output_submission(raw_test, prediction, id_column, prediction_column, file_name):
    df = pd.DataFrame(prediction, columns=[prediction_column])
    df[id_column] = raw_test[id_column]
    df[[id_column, prediction_column]].to_csv((file_name), index=False)
    print('Output complete')
    
    
def plot_loss_accuracy(history):
    plt.figure(figsize=(20,10))
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')
    plt.show()
    
TRAIN_PATH = 'train_set.csv'
TEST_PATH = 'test.csv'
SUBMISSION_NAME = 'submission.csv'

# Model parameters
BATCH_SIZE = 128
EPOCHS = 25
LEARNING_RATE = 0.0001
DATASET_SIZE = 1000000

# Only a fraction of the whole data
train = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE)
test = pd.read_csv(TEST_PATH)
# clean data
train = clean(train)
# process data
train = process(train)
test = process(test)

# process data
train = add_engineered(train)
test = add_engineered(test)

# Drop unwanted columns
train_clean = train.drop(['key', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 
                  'dropoff_latitude', 'passenger_count', 'latdiff', 'londiff'], axis=1)
test_clean = test.drop(['key', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 
                  'dropoff_latitude', 'passenger_count', 'latdiff', 'londiff'], axis=1)

train_clean = train_clean.astype('float64')
test_clean = test_clean.astype('float64')
# peek data
train_clean.head(5)

# split data in train and validation (80% ~ 10%)
train_df, validation_df = train_test_split(train_clean, test_size=0.1, random_state=1)

# Get labels
train_labels = train_df['fare_amount'].values
validation_labels = validation_df['fare_amount'].values
train_df = train_df.drop(['fare_amount'], axis=1)
validation_df = validation_df.drop(['fare_amount'], axis=1)
# Scale data
# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow
# Selecting only wanted columns before scaling
# wanted_columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 
#                   'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 
#                   'hour', 'weekday', 'latdiff', 'londiff', 'euclidean', 'ploc', 
#                   'dloc', 'pd_pair', 'day_hr']

# train_df_scaled = train_df[wanted_columns]
# validation_df_scaled = validation_df[wanted_columns]
# test_scaled = test[wanted_columns]

scaler = preprocessing.MinMaxScaler()
train_df_scaled = scaler.fit_transform(train_df)
validation_df_scaled = scaler.transform(validation_df)
test_scaled = scaler.transform(test_clean)

model = Sequential()
model.add(Dense(64, activation='relu', input_dim=train_df_scaled.shape[1]))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(8, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(1))

adam = optimizers.adam(lr=LEARNING_RATE)
model.compile(loss='mse', optimizer=adam, metrics=['mae'])
# early = EarlyStopping(monitor='val_loss', patience=15, mode='min')

history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, 
                    validation_data=(validation_df_scaled, validation_labels), shuffle=True)

plot_loss_accuracy(history)

# Make prediction
prediction = model.predict(test_scaled, batch_size=128, verbose=1)

# output prediction
output_submission(test, prediction, 'key', 'fare_amount', SUBMISSION_NAME)



































